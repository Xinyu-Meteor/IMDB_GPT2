{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "原权重推理进度: 100%|██████████| 2/2 [00:00<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原权重准确率: 0.94\n",
      "原权重推理时间: 0.38 秒\n",
      "原权重内存占用: 940.046875 MB\n",
      "原权重推理完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "import time  # 用于计算时间\n",
    "from memory_profiler import memory_usage  # 用于监控内存\n",
    "import numpy as np  # 用于计算准确率\n",
    "\n",
    "\n",
    "# 加载本地模型和 tokenizer\n",
    "model_name = '../gpt2-imdb-sentiment-classifier'  # 替换为您的模型路径\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 检查可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 2. 准备数据集\n",
    "data = pd.read_csv(\"../tinyimdb.csv\")  # 替换为你的数据集文件名\n",
    "texts = data['review'].tolist()  # 获取文本列表\n",
    "true_labels = data['sentiment'].tolist()  # 假设真实标签在'sentiment'列中\n",
    "\n",
    "# 3. 将真实标签映射到数字\n",
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 4. 进行推理\n",
    "# 将输入数据分批次处理以适应 GPU 内存\n",
    "batch_size = 16  # 根据您的 GPU 内存大小调整\n",
    "predictions = []\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 禁用梯度计算以提高推理效率\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()  # 开始时间\n",
    "    \n",
    "    # 使用 tqdm 显示进度条\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"原权重推理进度\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # 进行推理\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 获取预测结果\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()  # 转换为numpy数组并移回CPU\n",
    "        predictions.extend(batch_predictions)  # 将批次结果添加到总预测结果中\n",
    "\n",
    "    end_time = time.time()  # 结束时间\n",
    "\n",
    "# 5. 处理结果\n",
    "data['predictions'] = predictions  # 将预测结果添加到数据集中\n",
    "\n",
    "# 6. 计算准确率\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels_mapped))  # 计算准确率\n",
    "print(f\"原权重准确率: {accuracy:.2f}\")\n",
    "\n",
    "# 7. 计算推理时间\n",
    "inference_time = end_time - start_time\n",
    "print(f\"原权重推理时间: {inference_time:.2f} 秒\")\n",
    "\n",
    "# 8. 监控内存使用情况\n",
    "mem_usage = memory_usage(proc=None, interval=0.1, timeout=1)\n",
    "print(f\"原权重内存占用: {max(mem_usage)} MB\")\n",
    "\n",
    "print(\"原权重推理完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小数据贪婪策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "原权重推理进度: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原权重准确率: 0.94\n",
      "稀疏权重准确率: 1.00\n",
      "推理时间: 0.39 秒\n",
      "内存占用: 996.55078125 MB\n",
      "推理完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "import time  # 用于计算时间\n",
    "from memory_profiler import memory_usage  # 用于监控内存\n",
    "import numpy as np  # 用于计算准确率\n",
    "\n",
    "# 定义稀疏化权重的函数\n",
    "def greedy_sparse_weights(weights, activation_ratio=0.5):\n",
    "    abs_weights = torch.abs(weights)\n",
    "    num_weights = weights.numel()\n",
    "    num_active = int(num_weights * activation_ratio)\n",
    "    _, indices = torch.topk(abs_weights.view(-1), num_active)\n",
    "    sparse_weights = torch.zeros_like(weights)\n",
    "    sparse_weights.view(-1)[indices] = weights.view(-1)[indices]\n",
    "    return sparse_weights\n",
    "\n",
    "# 加载本地模型和 tokenizer\n",
    "model_name = '../gpt2-imdb-sentiment-classifier'  # 替换为您的模型路径\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 检查可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 复制模型\n",
    "model_sparse = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "model_sparse.to(device)  # 移动到同一设备\n",
    "\n",
    "# 2. 准备数据集\n",
    "data = pd.read_csv(\"../tinyimdb.csv\")  # 替换为你的数据集文件名\n",
    "texts = data['review'].tolist()  # 获取文本列表\n",
    "true_labels = data['sentiment'].tolist()  # 假设真实标签在'sentiment'列中\n",
    "\n",
    "# 3. 将真实标签映射到数字\n",
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 4. 进行推理\n",
    "# 将输入数据分批次处理以适应 GPU 内存\n",
    "batch_size = 16  # 根据您的 GPU 内存大小调整\n",
    "predictions = []\n",
    "predictions_sparse = []  # 用于存储稀疏模型的预测\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "model_sparse.eval()\n",
    "\n",
    "# 禁用梯度计算以提高推理效率\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()  # 开始时间\n",
    "    \n",
    "    # 使用 tqdm 显示进度条\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"原权重推理进度\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # 进行原模型推理\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()  # 转换为numpy数组并移回CPU\n",
    "        predictions.extend(batch_predictions)  # 将批次结果添加到总预测结果中\n",
    "\n",
    "        # 对稀疏模型的权重进行稀疏化\n",
    "        for layer in model_sparse.transformer.h:\n",
    "            layer.attn.c_attn.weight.data = greedy_sparse_weights(layer.attn.c_attn.weight.data)\n",
    "\n",
    "        # 进行稀疏模型推理\n",
    "        outputs_sparse = model_sparse(**inputs)\n",
    "        logits_sparse = outputs_sparse.logits\n",
    "        batch_predictions_sparse = torch.argmax(logits_sparse, dim=1).cpu().numpy()  # 转换为numpy数组并移回CPU\n",
    "        predictions_sparse.extend(batch_predictions_sparse)  # 将批次结果添加到稀疏模型的总预测结果中\n",
    "\n",
    "    end_time = time.time()  # 结束时间\n",
    "\n",
    "# 5. 处理结果\n",
    "data['predictions'] = predictions  # 将原模型的预测结果添加到数据集中\n",
    "data['predictions_sparse'] = predictions_sparse  # 将稀疏模型的预测结果添加到数据集中\n",
    "\n",
    "# 6. 计算准确率\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels_mapped))  # 计算原模型准确率\n",
    "accuracy_sparse = np.mean(np.array(predictions_sparse) == np.array(true_labels_mapped))  # 计算稀疏模型准确率\n",
    "print(f\"原权重准确率: {accuracy:.2f}\")\n",
    "print(f\"稀疏权重准确率: {accuracy_sparse:.2f}\")\n",
    "\n",
    "# 7. 计算推理时间\n",
    "inference_time = end_time - start_time\n",
    "print(f\"推理时间: {inference_time:.2f} 秒\")\n",
    "\n",
    "# 8. 监控内存使用情况\n",
    "mem_usage = memory_usage(proc=None, interval=0.1, timeout=1)\n",
    "print(f\"内存占用: {max(mem_usage)} MB\")\n",
    "\n",
    "print(\"推理完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小数据随机策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "随机稀疏推理进度:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([16, 512, 512])\n",
      "Y2.shape torch.Size([16, 512, 512])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "随机稀疏推理进度:  50%|█████     | 1/2 [00:00<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "随机稀疏推理进度: 100%|██████████| 2/2 [00:00<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Processing Layer GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")...\n",
      "Y1.shape torch.Size([1, 166, 166])\n",
      "Y2.shape torch.Size([1, 166, 166])\n",
      "Condition met for Layer: GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D(nf=2304, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=768)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D(nf=3072, nx=768)\n",
      "    (c_proj): Conv1D(nf=768, nx=3072)\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "随机稀疏权重准确率: 0.47\n",
      "随机稀疏推理时间: 0.83 秒\n",
      "全激活层数量: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机稀疏权重内存占用: 1050.33984375 MB\n",
      "随机稀疏推理完成\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "import time  # 用于计算时间\n",
    "from memory_profiler import memory_usage  # 用于监控内存\n",
    "import numpy as np  # 用于计算准确率\n",
    "\n",
    "def frobenius_norm(tensor):\n",
    "    return torch.norm(tensor, p='fro')\n",
    "\n",
    "def sparse_weights(weights, activation_ratio=0):\n",
    "    # 随机选择70%的权重进行激活\n",
    "    num_weights = weights.numel()\n",
    "    num_active = int(num_weights * activation_ratio)\n",
    "    \n",
    "    # 获取所有权重的索引\n",
    "    indices = torch.randperm(num_weights)[:num_active]\n",
    "    \n",
    "    # 创建一个全零的权重矩阵\n",
    "    sparse_weights = torch.zeros_like(weights)\n",
    "    \n",
    "    # 激活随机选择的权重\n",
    "    sparse_weights.view(-1)[indices] = weights.view(-1)[indices]\n",
    "    return sparse_weights\n",
    "\n",
    "def compute_outer_product(y1):\n",
    "    # y1 的形状应为 (batch_size, sequence_length, hidden_size)\n",
    "    batch_size, seq_length, hidden_size = y1.shape\n",
    "    \n",
    "    # 初始化结果张量\n",
    "    outer_product = torch.zeros((batch_size, seq_length, seq_length), device=y1.device)\n",
    "\n",
    "    # 对于每个样本，计算外积\n",
    "    for b in range(batch_size):\n",
    "        outer_product[b] = torch.matmul(y1[b], y1[b].transpose(0, 1))  # 计算外积\n",
    "\n",
    "    return outer_product\n",
    "\n",
    "def compute_inner_product(y1, y2):\n",
    "    # 确保 y1 和 y2 的形状相同\n",
    "    assert y1.shape == y2.shape, \"y1 and y2 must have the same shape\"\n",
    "    \n",
    "    batch_size, seq_length, hidden_size = y1.shape\n",
    "    \n",
    "    # 初始化结果张量\n",
    "    inner_product = torch.zeros((batch_size, seq_length, seq_length), device=y1.device)\n",
    "\n",
    "    # 对于每个样本，计算内积\n",
    "    for b in range(batch_size):\n",
    "        inner_product[b] = torch.matmul(y1[b], y2[b].transpose(0, 1))  # 计算内积\n",
    "\n",
    "    return inner_product\n",
    "\n",
    "\n",
    "\n",
    "# 自定义模型推理带随机稀疏化\n",
    "def model_inference_with_random_sparsity(model_original, model_sparse, input_ids):\n",
    "    hidden_states = model_original.transformer.wte(input_ids)  # 获取输入的嵌入表示\n",
    "    full_activation_count = 0  # 全激活层计数\n",
    "\n",
    "    for layer_original, layer_sparse in zip(model_original.transformer.h, model_sparse.transformer.h):\n",
    "        print(f\"Processing Layer {layer_original}...\")\n",
    "\n",
    "        # 原始权重\n",
    "        weights = layer_original.attn.c_attn.weight\n",
    "\n",
    "        # 计算 y1\n",
    "        batch_size, seq_length = hidden_states.size(0), hidden_states.size(1)\n",
    "        y1 = layer_original.attn(hidden_states)[0]  # 当前层的注意力机制得到输出\n",
    "        Y1 = compute_outer_product(y1)\n",
    "        print('Y1.shape',Y1.shape)\n",
    "\n",
    "        attempts = 0\n",
    "        while attempts < 20:\n",
    "            attempts += 1\n",
    "\n",
    "            # 随机稀疏化权重\n",
    "            sparse_weights_matrix = sparse_weights(weights)  # 使用随机稀疏化权重\n",
    "            \n",
    "            # 将稀疏化后的权重应用到复制的模型中\n",
    "            layer_sparse.attn.c_attn.weight.data = sparse_weights_matrix\n",
    "\n",
    "            # 使用复制模型进行推理\n",
    "            y2 = layer_sparse.attn(hidden_states)[0]  # 使用稀疏化后的权重计算输出\n",
    "            Y2 = compute_inner_product(y1, y2)\n",
    "            print('Y2.shape', Y2.shape)\n",
    "\n",
    "            # 计算 Frobenius 范数\n",
    "            frobenius_Y1 = frobenius_norm(Y1)\n",
    "            frobenius_Y2 = frobenius_norm(Y2)\n",
    "\n",
    "            # 检查条件\n",
    "            if frobenius_Y2 >= 0 * frobenius_Y1:\n",
    "                print(\"Condition met for Layer:\", layer_original)\n",
    "                hidden_states = y2  # 使用稀疏化后的输出\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempts}: Condition not met, retrying...\")\n",
    "\n",
    "        '''if attempts == 20:\n",
    "            print(\"Max attempts reached without meeting condition.\")\n",
    "            hidden_states = y1  # 如果不满足条件，使用原始输出\n",
    "            full_activation_count += 1  # 全激活层计数增加'''\n",
    "\n",
    "    return hidden_states, full_activation_count  # 返回经过所有层的最终输出和全激活层计数\n",
    "\n",
    "# 加载本地模型和 tokenizer\n",
    "model_name = '../gpt2-imdb-sentiment-classifier'  # 替换为您的模型路径\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 检查可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 复制模型\n",
    "model_sparse = GPT2ForSequenceClassification.from_pretrained(model_name)\n",
    "model_sparse.to(device)  # 移动到同一设备\n",
    "\n",
    "# 准备数据集\n",
    "data = pd.read_csv(\"../tinyimdb.csv\")  # 替换为你的数据集文件名\n",
    "texts = data['review'].tolist()  # 获取文本列表\n",
    "true_labels = data['sentiment'].tolist()  # 假设真实标签在'sentiment'列中\n",
    "\n",
    "# 将真实标签映射到数字\n",
    "label_mapping = {'negative': 0, 'positive': 1}\n",
    "true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
    "\n",
    "# 推理\n",
    "batch_size = 16  # 根据您的 GPU 内存大小调整\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()  # 开始时间\n",
    "    \n",
    "    full_activation_count_total = 0  # 全激活层计数总和\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"随机稀疏推理进度\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # 使用随机稀疏化进行推理\n",
    "        hidden_states, full_activation_count = model_inference_with_random_sparsity(model, model_sparse, inputs['input_ids'])\n",
    "        full_activation_count_total += full_activation_count\n",
    "        \n",
    "        # 计算 logits\n",
    "        logits = model_sparse.forward(inputs['input_ids']).logits  # 通过模型的 forward 方法计算 logits\n",
    "        batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()  # 转换为numpy数组并移回CPU\n",
    "        predictions.extend(batch_predictions)  # 将批次结果添加到总预测结果中\n",
    "\n",
    "    end_time = time.time()  # 结束时间\n",
    "\n",
    "# 处理结果\n",
    "data['predictions'] = predictions  # 将预测结果添加到数据集中\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels_mapped))  # 计算准确率\n",
    "print(f\"随机稀疏权重准确率: {accuracy:.2f}\")\n",
    "\n",
    "# 计算推理时间\n",
    "inference_time = end_time - start_time\n",
    "print(f\"随机稀疏推理时间: {inference_time:.2f} 秒\")\n",
    "\n",
    "# 输出全激活层数量\n",
    "print(f\"全激活层数量: {full_activation_count_total}\")\n",
    "\n",
    "# 监控内存使用情况\n",
    "mem_usage = memory_usage(proc=None, interval=0.1, timeout=1)\n",
    "print(f\"随机稀疏权重内存占用: {max(mem_usage)} MB\")\n",
    "\n",
    "print(\"随机稀疏推理完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imdb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
